#!/bin/bash

set -x

SQL_TYPE=$1
PLATFORM=$2
PLATFORM_VERSION=$3
SQL_ENGINE=$4
DATA_WAREHOUSE_NAME=$5
CONN_ID=$6
CONNECTOR_TYPE=$7
KERB_PRINCIPAL=$8
RPC_PROTECTION=$9
QUERY_ROLES=${10}
DISABLE_JDBC=${11}

DEFAULT_ORG_ID='default'

function version_gt() { test "$(printf '%s\n' "$@" | sort -V | head -n 1)" != "$1"; }
function version_lt() { test "$(printf '%s\n' "$@" | sort -V | tail -n 1)" != "$1"; }

case "$SQL_TYPE" in
  disable)
    mv {{ atscale_install_target}}/conf/supervisor/program_atscale-hiveserver2.conf {{ atscale_install_target}}/conf/supervisor/program_atscale-hiveserver2.disabled
    mv {{ atscale_install_target}}/conf/supervisor/program_atscale-spark.conf {{ atscale_install_target}}/conf/supervisor/program_atscale-spark.disabled

    {{ atscale_install_target }}/current/bin/database/postgres_psql -f <( cat<<ENDL
update atscale.connection_subgroups set name = 'interactive' where name = 'atscale-sparksql-interactive';
update atscale.connection_subgroups set name = 'system' where name = 'atscale-hive-system';
ENDL
    )

    exit
    ;;
  interactive)
    SQL_ENGINE_NAME="interactive"
    ;;
  batch)
    SQL_ENGINE_NAME="system"
    ;;
  *)
    echo "$SQL_TYPE isn't a supported sql type"
    exit 1
    ;;
esac

{% if secured == "yes" %}
if [ -n "${DISABLE_JDBC}" ]; then
  TLS_JDBC_STRING=""
else
  TLS_JDBC_STRING="{{ ssl_hive_jdbc_string }}"
fi

IMPALA_JDBC_STRING="{{ impala_secure_jdbc_string }}${TLS_JDBC_STRING}"
if [ "$PLATFORM" == "mapr" ]; then
  HIVE_JDBC_STRING="{{ hive_mapr_secure_jdbc_string }}${TLS_JDBC_STRING}"
# Any HDP version < 3.0 (where hive is < 2) wont support Kerb + TLS strings
# Dont wrap the version_lt part in square brackets '[' It doesn't need it and causes errors
elif [ "$PLATFORM" == "hdp" ] && version_lt "${PLATFORM_VERSION}" "3.0.0"; then
  HIVE_JDBC_STRING="{{ hive_secure_jdbc_string }}"
  SPARK_JDBC_STRING="{{ hive_secure_jdbc_string }}${TLS_JDBC_STRING}"
else
  HIVE_JDBC_STRING="{{ hive_secure_jdbc_string }}${TLS_JDBC_STRING}"
fi
{% else %}
IMPALA_JDBC_STRING="{{ impala_jdbc_string }}"
HIVE_JDBC_STRING="{{ hive_jdbc_string }}"
{% endif %}

if [ "$PLATFORM" == "cdh" ]; then
  # Dont wrap the version_gt part in square brackets '[' It doesn't need it and causes errors
  if [ -n "${PLATFORM_VERSION}" ] && version_gt "${PLATFORM_VERSION}" "6.0.0"; then
    HIVE_CONNECTOR="hive"
  else
    {% if secured == "yes" %}
    if [ -n "${DISABLE_JDBC}" ]; then
      HIVE_CONNECTOR="hive1"
    else
      HIVE_CONNECTOR="hive1cdh5"
      HIVE_JDBC_STRING="${HIVE_JDBC_STRING};kerberosAuthType=fromSubject"
    fi
    {% else %}
    HIVE_CONNECTOR="hive1"
    {% endif %}
  fi
else
  HIVE_CONNECTOR="hive"
fi


case "$SQL_ENGINE" in
  cdh-impala)
    SQL_ENGINE_HOST="{{ hostname_hadoop }}"
    SQL_ENGINE_PORT="21050"
    SQL_ENGINE_PROTOCOL=${CONNECTOR_TYPE:-"hive"}
    SQL_ENGINE_JDBCFLAGS="${IMPALA_JDBC_STRING}"
    SQL_ENGINE_EXTRA_PROPS=""
    UDAF_MODE="engine_managed"
    ;;
  cdh-impala-3)
    SQL_ENGINE_HOST="{{ hostname_hadoop | regex_replace('-01','-0[1-3]') }}"
    SQL_ENGINE_PORT="21050"
    SQL_ENGINE_PROTOCOL=${CONNECTOR_TYPE:-"hive"}
    SQL_ENGINE_JDBCFLAGS="${IMPALA_JDBC_STRING}"
    SQL_ENGINE_EXTRA_PROPS=""
    UDAF_MODE="engine_managed"
    ;;
  hive)
    SQL_ENGINE_HOST="{{ hostname_hadoop }}"
    SQL_ENGINE_PORT="10000"
    SQL_ENGINE_PROTOCOL=${CONNECTOR_TYPE:-$HIVE_CONNECTOR}
    SQL_ENGINE_JDBCFLAGS="${HIVE_JDBC_STRING}"
    UDAF_MODE="engine_managed"
    ;;
  spark)
    SQL_ENGINE_HOST="{{ hostname_hadoop }}"
    SQL_ENGINE_PORT="10015"
    SQL_ENGINE_PROTOCOL=${CONNECTOR_TYPE:-$HIVE_CONNECTOR}
    SQL_ENGINE_JDBCFLAGS="${SPARK_JDBC_STRING:-$HIVE_JDBC_STRING}"
    SQL_ENGINE_EXTRA_PROPS=""
    UDAF_MODE="engine_managed"
    ;;
  cdh-hive)
    SQL_ENGINE_HOST="{{ hostname_hadoop }}"
    SQL_ENGINE_PORT="10000"
    SQL_ENGINE_PROTOCOL=${CONNECTOR_TYPE:-$HIVE_CONNECTOR}
    SQL_ENGINE_JDBCFLAGS="${HIVE_JDBC_STRING}"
    SQL_ENGINE_EXTRA_PROPS=""
    UDAF_MODE="engine_managed"
    ;;
  as-hive)
    if [ -f "{{ atscale_install_target}}/conf/supervisor/program_atscale-hiveserver2.disabled" ]; then
      mv {{ atscale_install_target}}/conf/supervisor/program_atscale-hiveserver2.disabled {{ atscale_install_target}}/conf/supervisor/program_atscale-hiveserver2.conf
      {{ atscale_install_target }}/bin/atscale_service_control update atscale-hiveserver2

      {{ image_util_dir }}/set-atscale-param.sh tez tez.cluster.additional.classpath.prefix $(hadoop classpath)
    fi
    SQL_ENGINE_HOST="{{ hostname_atscale }}"
    SQL_ENGINE_PORT="10511"
    SQL_ENGINE_PROTOCOL=${CONNECTOR_TYPE:-"hive"}
    {% if secured == "yes" %}
    SQL_ENGINE_JDBCFLAGS=";principal={{ atscale_user }}/{{ atscale_public_server_name }}@{{ krb_realm }}"
    {% else %}
    SQL_ENGINE_JDBCFLAGS=""
    {% endif %}
    SQL_ENGINE_EXTRA_PROPS=""
    UDAF_MODE="engine_managed"
    ;;
  as-spark)
    if [ -f "{{ atscale_install_target}}/conf/supervisor/program_atscale-spark.disabled" ]; then
      mv {{ atscale_install_target}}/conf/supervisor/program_atscale-spark.disabled {{ atscale_install_target}}/conf/supervisor/program_atscale-spark.conf
      {{ atscale_install_target }}/bin/atscale_service_control update atscale-spark

      {{ image_util_dir }}/set-atscale-param.sh spark-env SPARK_DIST_CLASSPATH $(hadoop classpath)
    fi
    SQL_ENGINE_HOST="{{ hostname_atscale }}"
    SQL_ENGINE_PORT="10510"
    SQL_ENGINE_PROTOCOL=${CONNECTOR_TYPE:-"hive"}
    {% if secured == "yes" %}
    SQL_ENGINE_JDBCFLAGS=";principal={{ atscale_user }}/{{ atscale_public_server_name }}@{{ krb_realm }}"
    {% else %}
    SQL_ENGINE_JDBCFLAGS=""
    {% endif %}
    SQL_ENGINE_EXTRA_PROPS=""
    UDAF_MODE="engine_managed"
    ;;
  presto)
    SQL_ENGINE_HOST="{{ hostname_presto }}"
    SQL_ENGINE_PORT="8080"
    SQL_ENGINE_PROTOCOL=${CONNECTOR_TYPE:-"presto"}
    {% if secured == "yes" %}
    SQL_ENGINE_JDBCFLAGS=";principal=presto/{{ cluster_master_hostname }}@{{ krb_realm }};ssl=true;sslTrustStore={{ tls_certificate_dir }}/san-docker.infra.atscale.com.jks;trustStorePassword={{ tls_store_password }}"
    {% else %}
    SQL_ENGINE_JDBCFLAGS=""
    {% endif %}
    UDAF_MODE="customer_managed"

    DATE=$(date +%Y%m%d)
    DT=$(date +%Y%m%d%H%M)

    {% if as_version is version_compare('7.4', '<') %}
      [ "$SQL_ENGINE_NAME" == "system" ] && ID_TICK=1 || ID_TICK=2

        {{ atscale_install_target }}/current/bin/database/postgres_psql -f <( cat<<ENDL
          update properties set property_value = true where property_name = 'feature.connection.engine.protocol.presto';

          insert into connection_subgroup_extra_props (id, connection_subgroup_id, property_name, property_value, created_at)
          values ('$DATE-3333-3333-000${ID_TICK}-$DT',
                  (select connection_subgroup_id from connection_subgroups where name = '$SQL_ENGINE_NAME'),
                  'atscaleHoneybeeInstalled',
                  true,
                  now() );
ENDL
        )
    {% else %}
      SQL_ENGINE_EXTRA_PROPS='"atscaleHoneybeeInstalled":"true"'
    {% endif %}

    ;;
  *)
    echo "$SQL_ENGINE isn't a supported sql engine"
    exit 1
    ;;
esac

{% if as_version is version_compare('7.4', '>=') %}

{% if secured == "yes" %}
{% set protocol = "https" %}
jwt=`curl -s -k -u admin:admin https://localhost:10500/default/auth`
{% else %}
{% set protocol = "http" %}
jwt=`curl -s -u admin:admin http://localhost:10500/default/auth`
{% endif %}

cgid=`curl -s -k -H "Authorization:Bearer $jwt" {{ protocol }}://localhost:10502/connection-groups/orgId/$DEFAULT_ORG_ID/conn/$CONN_ID | jq -r .response.id`

echo "Existing connection group ID corresponding to connection ID $CONN_ID is $cgid"

if [ -z "$cgid" ]; then
   echo Failed to get the connection group ID corresponding to connection ID $CONN_ID
   exit 1
fi

if [ "$PLATFORM" == "mapr" ]; then
  URI="maprfs:///$SQL_ENGINE_HOST:8020/mapr/my.cluster.com"
  URI_KERB="mapr/$SQL_ENGINE_HOST@$KERB_PRINCIPAL"
  FS_TYPE="maprfs"
else
  URI="hdfs://$SQL_ENGINE_HOST:8020"
  URI_KERB="hdfs/$SQL_ENGINE_HOST@$KERB_PRINCIPAL"
  FS_TYPE="hdfs"
fi

{% if secured != "yes" %}
URI_KERB=""
{% endif %}

PUT_BODY=$(tr -d [:cntrl:] <<ENDL
  {
    "name" : "$DATA_WAREHOUSE_NAME",
    "connectionId" : "$CONN_ID",
    "filesystemType" : "$FS_TYPE",
    "filesystemUri" : "$URI",
    "aggregateSchema" : "atscaleagg",
    "extraProperties" : {
{% if secured == "yes" %}
      "hdfsNameNodeKerberosPrincipal" : "$URI_KERB",
      "hadoopRpcProtection" : "$RPC_PROTECTION",
{% endif %}
      "udafMode" : "$UDAF_MODE",
      "udafSchema" : "atscaleudaf",
      "fsArtifactInstallPath" : "/user/atscaler/atscale/engine"
    }
  }
ENDL
)

curl -f -s -k -X PUT -H "Authorization:Bearer $jwt" -H "Content-Type: application/json" --data "$PUT_BODY" {{ protocol }}://localhost:10502/connection-groups/orgId/$DEFAULT_ORG_ID/connection-group/$cgid > /dev/null
RET=$?

if [ $RET -ne 0 ]; then
  echo "Failed to add extra properties to the '$DATA_WAREHOUSE_NAME' connection group ($RET)."
  exit 1
fi


POST_BODY=$(tr -d [:cntrl:] <<ENDL
{
  "name" : "$SQL_ENGINE_NAME",
  "connectorType" : "$SQL_ENGINE_PROTOCOL",
  "hosts" : "$SQL_ENGINE_HOST",
  "port" : $SQL_ENGINE_PORT,
  "queryRoles" : $QUERY_ROLES,
  "extraJdbcFlags" : "$SQL_ENGINE_JDBCFLAGS",
  "extraProperties" : { $SQL_ENGINE_EXTRA_PROPS },
  "username" : "atscaler"
}
ENDL
)

curl -f -s -k -X POST -H "Authorization:Bearer $jwt" -H "Content-Type: application/json" --data "$POST_BODY" {{ protocol }}://localhost:10502/connection-groups/orgId/$DEFAULT_ORG_ID/connection-group/$cgid > /dev/null
RET=$?

if [ $RET -ne 0 ]; then
  echo "Failed to create SQL Engine '$SQL_ENGINE_NAME' with connector type $SQL_ENGINE_PROTOCOL ($RET)."
  exit 1
fi

{% else %}

# Temporary hack for MapR/CDH/HDP
if [[ ":mapr:hdp:cdh:" == *":$PLATFORM:"* ]]; then
  DATE=$(date +%Y%m%d)
  DT=$(date +%Y%m%d%H%M)
  ENV_UUID="prod"
  GROUP_UUID="10000000-0000-0000-0000-100000000000"
  SUBGROUP_UUID="$DATE-1111-1111-0000-$DT"
  GROUP_ROLESUUID="$DATE-2222-2222-0000-$DT"

  [ "$PLATFORM" == "mapr" ] && URI="maprfs:///$SQL_ENGINE_HOST:8020" || URI="hdfs://$SQL_ENGINE_HOST:8020"

  {{ atscale_install_target}}/current/bin/database/postgres_psql < <(cat <<ENDL
-- Environment
insert into environment (env_id, org_id, name, default_env, created_at, updated_at, hiveserver2_port )
values ( '$ENV_UUID', 'default', 'prod', 't', now(), now(), 11111 );

-- Connection group
insert into connection_groups ( org_id, env_id, conn_id, hdfs_uri, default_schema, id, name, database )
values ( 'default', '$ENV_UUID', 'con1', '$URI', 'atscale', '$GROUP_UUID', 'Hive Warehouse', null );

insert into connection_groups_environments( id, organization_id, environment_id, connection_group_id, created_at )
values ( '$DATE-3333-3333-0000-$DT', 'default', '$ENV_UUID', '$GROUP_UUID', now() );

-- Connection subgroups
insert into connection_subgroups( connection_subgroup_id, name, hosts, port, db_engine_protocol, username, password, connection_group_id )
values( '$SUBGROUP_UUID', '$SQL_ENGINE_NAME', '$SQL_ENGINE_HOST', '$SQL_ENGINE_PORT', '$SQL_ENGINE_PROTOCOL', 'atscaler', null, '$GROUP_UUID' );

insert into connection_subgroup_roles( id, connection_subgroup_id, role, created_at )
values
( '$DATE-4444-4444-0001-$DT', '$SUBGROUP_UUID', 'large_user_query_role', now() ),
( '$DATE-4444-4444-0002-$DT', '$SUBGROUP_UUID', 'small_user_query_role', now() ),
( '$DATE-4444-4444-0003-$DT', '$SUBGROUP_UUID', 'system_query_role', now() );

ENDL
)

fi

{{ atscale_install_target }}/current/bin/database/postgres_psql -f <( cat<<ENDL
update connection_subgroups
set hosts = '$SQL_ENGINE_HOST',
    port = '$SQL_ENGINE_PORT',
    db_engine_protocol = '$SQL_ENGINE_PROTOCOL',
    extra_jdbc_flags = '$SQL_ENGINE_JDBCFLAGS'
where name = '$SQL_ENGINE_NAME';
ENDL
)

{% endif %}
